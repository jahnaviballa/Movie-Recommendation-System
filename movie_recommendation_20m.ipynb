{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e3a5e15e-9ffe-4e13-b357-3ff8b6995447",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\jahna\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ratings shape: (1000000, 6)\n",
      "Movies shape: (15374, 5)\n",
      "Tags shape: (446487, 4)\n",
      "Genome scores shape: (11489808, 3)\n",
      "   userId  movieId  rating            timestamp  \\\n",
      "0  122270     8360     3.5  2012-04-22 01:07:04   \n",
      "1   49018       32     2.0  2001-09-11 07:50:36   \n",
      "2   89527   109374     3.5  2015-01-06 09:26:40   \n",
      "3  106704     1060     3.0  2000-01-22 21:27:57   \n",
      "4   47791     1732     2.0  2006-01-19 15:48:23   \n",
      "\n",
      "                                       title  \\\n",
      "0                             Shrek 2 (2004)   \n",
      "1  Twelve Monkeys (a.k.a. 12 Monkeys) (1995)   \n",
      "2           Grand Budapest Hotel, The (2014)   \n",
      "3                            Swingers (1996)   \n",
      "4                   Big Lebowski, The (1998)   \n",
      "\n",
      "                                             content  \n",
      "0   A d v e n t u r e | A n i m a t i o n | C h i...  \n",
      "1   M y s t e r y | S c i - F i | T h r i l l e r...  \n",
      "2   C o m e d y | D r a m a  amazing storytelling...  \n",
      "3   C o m e d y | D r a m a  funny Vince Vaughn f...  \n",
      "4   C o m e d y | C r i m e  black comedy drugs m...  \n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from surprise import SVD, Reader, Dataset\n",
    "from surprise.model_selection import train_test_split\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import os\n",
    "\n",
    "# Download NLTK data for sentiment analysis\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "# Set data path\n",
    "data_path = r'C:\\Users\\jahna\\Movie_Recommendations\\data\\movielens-20m-dataset'\n",
    "\n",
    "# Load datasets\n",
    "movies = pd.read_csv(os.path.join(data_path, 'movie.csv'))\n",
    "ratings = pd.read_csv(os.path.join(data_path, 'rating.csv'))\n",
    "tags = pd.read_csv(os.path.join(data_path, 'tag.csv'))\n",
    "genome_scores = pd.read_csv(os.path.join(data_path, 'genome_scores.csv'))\n",
    "genome_tags = pd.read_csv(os.path.join(data_path, 'genome_tags.csv'))\n",
    "\n",
    "# Sample a subset for faster processing (optional, remove for full dataset)\n",
    "ratings = ratings.sample(n=1000000, random_state=42)  # Sample 1M ratings\n",
    "movies = movies[movies['movieId'].isin(ratings['movieId'].unique())]\n",
    "tags = tags[tags['movieId'].isin(ratings['movieId'].unique())]\n",
    "genome_scores = genome_scores[genome_scores['movieId'].isin(ratings['movieId'].unique())]\n",
    "\n",
    "# Clean data\n",
    "# Remove movies with missing titles\n",
    "movies = movies.dropna(subset=['title'])\n",
    "\n",
    "# Process genres\n",
    "movies['genres'] = movies['genres'].replace('|', ' ', regex=True)\n",
    "movies['genres'] = movies['genres'].replace('(no genres listed)', '')\n",
    "\n",
    "# Process tags\n",
    "# Aggregate user tags per movie\n",
    "movie_tags = tags.groupby('movieId')['tag'].apply(lambda x: ' '.join(x.astype(str))).reset_index()\n",
    "movies = movies.merge(movie_tags, on='movieId', how='left')\n",
    "movies['tag'] = movies['tag'].fillna('')\n",
    "\n",
    "# Combine genres and tags for content-based filtering\n",
    "movies['content'] = movies['genres'] + ' ' + movies['tag']\n",
    "\n",
    "# Merge ratings with movie titles\n",
    "ratings = ratings.merge(movies[['movieId', 'title', 'content']], left_on='movieId', right_on='movieId', how='left')\n",
    "\n",
    "# Display data\n",
    "print(\"Ratings shape:\", ratings.shape)\n",
    "print(\"Movies shape:\", movies.shape)\n",
    "print(\"Tags shape:\", tags.shape)\n",
    "print(\"Genome scores shape:\", genome_scores.shape)\n",
    "print(ratings.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6585efe6-3907-4ec3-a927-6741e6472b20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collaborative Filtering Recommendations for User 122270:\n",
      "      movieId                             title  \\\n",
      "312       318  Shawshank Redemption, The (1994)   \n",
      "1029     1069           Murder, My Sweet (1944)   \n",
      "1101     1147         When We Were Kings (1996)   \n",
      "1171     1221    Godfather: Part II, The (1974)   \n",
      "2426     2571                Matrix, The (1999)   \n",
      "\n",
      "                                                content  \n",
      "312    C r i m e | D r a m a  friendship masterplan ...  \n",
      "1029   C r i m e | F i l m - N o i r | T h r i l l e...  \n",
      "1101   D o c u m e n t a r y  character based on rea...  \n",
      "1171   C r i m e | D r a m a  complex characters maf...  \n",
      "2426   A c t i o n | S c i - F i | T h r i l l e r  ...  \n"
     ]
    }
   ],
   "source": [
    "# Prepare data for Surprise\n",
    "reader = Reader(rating_scale=(0.5, 5.0))\n",
    "data = Dataset.load_from_df(ratings[['userId', 'movieId', 'rating']], reader)\n",
    "\n",
    "# Split data into train and test sets\n",
    "trainset, testset = train_test_split(data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train SVD model\n",
    "svd = SVD(n_factors=100, random_state=42)\n",
    "svd.fit(trainset)\n",
    "\n",
    "# Function to get collaborative filtering predictions\n",
    "def get_collaborative_recommendations(user_id, n=5):\n",
    "    # Get all movie IDs\n",
    "    movie_ids = movies['movieId'].unique()\n",
    "    # Predict ratings for all movies\n",
    "    predictions = [svd.predict(user_id, movie_id) for movie_id in movie_ids]\n",
    "    # Sort by predicted rating\n",
    "    predictions.sort(key=lambda x: x.est, reverse=True)\n",
    "    # Get top N movie IDs\n",
    "    top_movie_ids = [pred.iid for pred in predictions[:n]]\n",
    "    # Get movie titles\n",
    "    top_movies = movies[movies['movieId'].isin(top_movie_ids)][['movieId', 'title', 'content']]\n",
    "    return top_movies\n",
    "\n",
    "# Test collaborative filtering\n",
    "user_id = ratings['userId'].iloc[0]  # Example user\n",
    "collab_recs = get_collaborative_recommendations(user_id)\n",
    "print(f\"Collaborative Filtering Recommendations for User {user_id}:\")\n",
    "print(collab_recs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "97bb2c42-bba0-48f4-bbf0-f36fbc9836bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Content-Based Recommendations for Toy Story (1995):\n",
      "      movieId                  title  \\\n",
      "2954     3114     Toy Story 2 (1999)   \n",
      "2214     2355   Bug's Life, A (1998)   \n",
      "4650     4886  Monsters, Inc. (2001)   \n",
      "4964     5218         Ice Age (2002)   \n",
      "6024     6377    Finding Nemo (2003)   \n",
      "\n",
      "                                                content  \n",
      "2954   A d v e n t u r e | A n i m a t i o n | C h i...  \n",
      "2214   A d v e n t u r e | A n i m a t i o n | C h i...  \n",
      "4650   A d v e n t u r e | A n i m a t i o n | C h i...  \n",
      "4964   A d v e n t u r e | A n i m a t i o n | C h i...  \n",
      "6024   A d v e n t u r e | A n i m a t i o n | C h i...  \n"
     ]
    }
   ],
   "source": [
    "# Create TF-IDF matrix for content (genres + tags)\n",
    "tfidf = TfidfVectorizer(stop_words='english', max_features=5000)\n",
    "tfidf_matrix = tfidf.fit_transform(movies['content'])\n",
    "\n",
    "# Compute cosine similarity between movies\n",
    "cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
    "\n",
    "# Function to get content-based recommendations\n",
    "def get_content_recommendations(title, n=5):\n",
    "    # Get movie index\n",
    "    idx = movies[movies['title'] == title].index\n",
    "    if len(idx) == 0:\n",
    "        return pd.DataFrame()\n",
    "    idx = idx[0]\n",
    "    # Get similarity scores\n",
    "    sim_scores = list(enumerate(cosine_sim[idx]))\n",
    "    # Sort by similarity\n",
    "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
    "    # Get top N similar movies\n",
    "    sim_scores = sim_scores[1:n+1]  # Exclude the movie itself\n",
    "    movie_indices = [i[0] for i in sim_scores]\n",
    "    return movies.iloc[movie_indices][['movieId', 'title', 'content']]\n",
    "\n",
    "# Test content-based filtering\n",
    "movie_title = movies['title'].iloc[0]  # Example movie\n",
    "content_recs = get_content_recommendations(movie_title)\n",
    "print(f\"Content-Based Recommendations for {movie_title}:\")\n",
    "print(content_recs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7fe86311-efce-4aff-b9f6-b1309a1073df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collaborative Recommendations with Positive Sentiment:\n",
      "Empty DataFrame\n",
      "Columns: [movieId, title, content, sentiment]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "# Initialize sentiment analyzer\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Compute sentiment for tags\n",
    "tags['sentiment'] = tags['tag'].apply(lambda x: sia.polarity_scores(str(x))['compound'])\n",
    "\n",
    "# Aggregate sentiment per movie\n",
    "movie_sentiment = tags.groupby('movieId')['sentiment'].mean().reset_index()\n",
    "\n",
    "# Function to filter recommendations by sentiment\n",
    "def filter_by_sentiment(recommendations, min_sentiment=0.1):\n",
    "    # Merge recommendations with sentiment\n",
    "    recs_with_sentiment = recommendations.merge(movie_sentiment, on='movieId', how='left')\n",
    "    # Fill missing sentiment with neutral (0)\n",
    "    recs_with_sentiment['sentiment'] = recs_with_sentiment['sentiment'].fillna(0)\n",
    "    # Filter by positive sentiment\n",
    "    return recs_with_sentiment[recs_with_sentiment['sentiment'] >= min_sentiment]\n",
    "\n",
    "# Test sentiment filtering on collaborative recommendations\n",
    "collab_recs_sentiment = filter_by_sentiment(collab_recs)\n",
    "print(\"Collaborative Recommendations with Positive Sentiment:\")\n",
    "print(collab_recs_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9ecc2d6d-24d1-421c-9300-4348f82a6d91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hybrid Recommendations for User 122270 and Movie Toy Story (1995):\n",
      "    movieId                           title  \\\n",
      "1       898  Philadelphia Story, The (1940)   \n",
      "12     4886           Monsters, Inc. (2001)   \n",
      "\n",
      "                                              content  sentiment  \n",
      "1    C o m e d y | D r a m a | R o m a n c e  scre...   0.193295  \n",
      "12   A d v e n t u r e | A n i m a t i o n | C h i...   0.136745  \n"
     ]
    }
   ],
   "source": [
    "# Function for hybrid recommendations\n",
    "def get_hybrid_recommendations(user_id, movie_title, n=5, use_sentiment=True):\n",
    "    # Get collaborative recommendations\n",
    "    collab_recs = get_collaborative_recommendations(user_id, n=10)\n",
    "    # Get content-based recommendations\n",
    "    content_recs = get_content_recommendations(movie_title, n=10)\n",
    "    # Combine recommendations\n",
    "    combined = pd.concat([collab_recs, content_recs]).drop_duplicates(subset=['movieId'])\n",
    "    # Apply sentiment filtering if enabled\n",
    "    if use_sentiment:\n",
    "        combined = filter_by_sentiment(combined)\n",
    "    # Return top N\n",
    "    return combined.head(n)\n",
    "\n",
    "# Test hybrid recommendations\n",
    "user_id = ratings['userId'].iloc[0]\n",
    "movie_title = movies['title'].iloc[0]\n",
    "hybrid_recs = get_hybrid_recommendations(user_id, movie_title)\n",
    "print(f\"Hybrid Recommendations for User {user_id} and Movie {movie_title}:\")\n",
    "print(hybrid_recs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
